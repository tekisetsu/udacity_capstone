{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic light detection in real word\n",
    "\n",
    "The goal of this jupyter notebook is to test the 3rd solution. Further exaplanations are provided in the README file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## library imports, env set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Model and label imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the research folder to the system path]\n",
    "script_dir = os.getcwd()\n",
    "research_dir = os.path.join(script_dir, \"research\")\n",
    "slim_dir = os.path.join(research_dir, \"slim\")\n",
    "\n",
    "sys.path.append(research_dir)\n",
    "sys.path.append(slim_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Load Frozen Model\n",
    "frozen_model_file = \"./model/frozen_inference_graph.pb\"\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(frozen_model_file, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "\n",
    "# Load labels\n",
    "current_dir = os.getcwd()\n",
    "labels_file = os.path.join(current_dir, \"model/mscoco_label_map.pbtxt\")\n",
    "category_index = label_map_util.create_category_index_from_labelmap(labels_file, use_display_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "                \n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "              'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on taken pictures, cropping the traffic light and storing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = './rosbag_pictures'\n",
    "IMAGE_SIZE = (12, 8) # Size, in inches, of the output images.\n",
    "\n",
    "\n",
    "def get_images_filenames(images_folder):\n",
    "    images_path = []\n",
    "    for filename in os.listdir(images_folder):\n",
    "        if \".jpg\" in filename:\n",
    "            images_path.append(os.path.join(images_folder, filename))\n",
    "    return images_path\n",
    "\n",
    "TEST_IMAGE_PATHS = get_images_filenames(PATH_TO_TEST_IMAGES_DIR)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_detections': 2, 'detection_boxes': array([[0.3759398 , 0.4747519 , 0.43865952, 0.4921544 ],\n",
      "       [0.65210533, 0.        , 0.9959734 , 1.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ]], dtype=float32), 'detection_scores': array([0.8719962, 0.7843518, 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       ], dtype=float32), 'detection_classes': array([10,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "      dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "# this line of code prevents a bug on MACOS, if ran on a different os please comment\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "def get_traffic_light(cv2_image, output_dict):\n",
    "    TRAFFIC_LIGHT_CLASS = 10\n",
    "    TRAFFIC_LIGHT_MIN_SCORE = 0.80\n",
    "    \n",
    "    height, width, _ = cv2_image.shape\n",
    "    \n",
    "    traffic_light_score = 0\n",
    "    traffic_light_index = None\n",
    "    \n",
    "    contrains_traffic_light = np.nonzero(output_dict[\"detection_classes\"] == TRAFFIC_LIGHT_CLASS)\n",
    "    traffic_light_indexes = contrains_traffic_light[0]\n",
    "    \n",
    "    for tf_light_index in traffic_light_indexes:\n",
    "        current_score = output_dict['detection_scores'][tf_light_index]\n",
    "        if current_score >= TRAFFIC_LIGHT_MIN_SCORE and current_score >= traffic_light_score:\n",
    "            traffic_light_score = current_score\n",
    "            traffic_light_index = tf_light_index        \n",
    "    \n",
    "    # No traffic light has been found\n",
    "    if traffic_light_index is None:\n",
    "        return None\n",
    "    \n",
    "    # Traffic light found\n",
    "    detection_box = output_dict[\"detection_boxes\"][traffic_light_index]\n",
    "    (ymin, xmin, ymax, xmax) = (int(detection_box[0]*height), int(detection_box[1]*width),\n",
    "                                 int(detection_box[2]*height), int(detection_box[3]*width))\n",
    "    return cv2_image[ymin:ymax,xmin:xmax]\n",
    "\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    \n",
    "    image_name = os.path.basename(image_path)\n",
    "    image_name = image_name.split(\".\")[0]\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=3)\n",
    "    \n",
    "    # saving the image, going back to cv2 because I am used to their lib\n",
    "    boxed_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"./boxed_images/{}.jpg\".format(image_name), boxed_image)\n",
    "    \n",
    "    cv2_image = cv2.imread(image_path)\n",
    "    cropped_image = get_traffic_light(cv2_image, output_dict)\n",
    "    if cropped_image is not None:\n",
    "        cv2.imwrite(\"./traffic_lights/{}.jpg\".format(image_name), cropped_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic light classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_green_mask(img_hsv):\n",
    "    lower_green = np.array([40,10,10])\n",
    "    upper_green = np.array([90,255,255])\n",
    "    mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_red_mask(img_hsv):\n",
    "    \n",
    "    # red lower mask (0-10)\n",
    "    lower_red = np.array([20,1,150])\n",
    "    upper_red = np.array([30,120,255])\n",
    "    mask0 = cv2.inRange(img_hsv, lower_red, upper_red)\n",
    "\n",
    "    # Red upper mask\n",
    "    lower_red = np.array([170,50,50])\n",
    "    upper_red = np.array([180,255,255])\n",
    "    mask1 = cv2.inRange(img_hsv, lower_red, upper_red)\n",
    "    \n",
    "    # join my masks\n",
    "    mask = mask0 +mask1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_traffic_light_color(cv2_image):\n",
    "    \n",
    "    # Convert BGR to HSV\n",
    "    img_hsv = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2HSV)\n",
    "    height, width, _ = img_hsv.shape\n",
    "    \n",
    "    green_mask = get_green_mask(img_hsv)\n",
    "    red_mask = get_red_mask(img_hsv)\n",
    "    \n",
    "    dico = {\n",
    "        \"red\" : np.count_nonzero(red_mask[0:int(height/3),:]),\n",
    "        \"yellow\" : np.count_nonzero(red_mask[int(height/3):int(height*2/3),:]),\n",
    "        \"green\" : np.count_nonzero(green_mask[int(height*2/3):height,:])\n",
    "    }\n",
    "    \n",
    "    v=list(dico.values())\n",
    "    k=list(dico.keys())\n",
    "    return k[v.index(max(v))]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing traffic light classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_light_dir = \"./traffic_lights\"\n",
    "traffic_light_images = get_images_filenames(traffic_light_dir)\n",
    "\n",
    "for traffic_light_image in traffic_light_images:\n",
    "    cv2_image = cv2.imread(traffic_light_image)\n",
    "    color = get_traffic_light_color(cv2_image)\n",
    "    if color not in traffic_light_image:\n",
    "        print(\"misclassified: {}, predicted color = {}\".format(traffic_light_image, color))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_traffic_light = \"./traffic_lights/green_1.jpg\"\n",
    "yellow_traffic_light = \"./traffic_lights/yellow_3.jpg\"\n",
    "red_traffic_light =\"./traffic_lights/red_5.jpg\"\n",
    "\n",
    "# image picking\n",
    "# cv2_image = cv2.imread(green_traffic_light)\n",
    "cv2_image = cv2.imread(yellow_traffic_light)\n",
    "# cv2_image = cv2.imread(red_traffic_light)\n",
    "\n",
    "img_hsv = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# green mask\n",
    "mask = get_green_mask(img_hsv)\n",
    "\n",
    "# red mask\n",
    "# mask = get_red_mask(img_hsv)\n",
    "\n",
    "# set my output img to zero everywhere except my mask\n",
    "output_img = cv2_image.copy()\n",
    "output_img[np.where(mask==0)] = 0\n",
    "\n",
    "# cv2.imwrite(\"./yellow_1_filtered_red.jpg\", output_img)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "height, width, _ = img_hsv.shape\n",
    "print(np.count_nonzero(mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_cv2 = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "# cv2.imshow('cropped',traffic_light)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(traffic_light, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
